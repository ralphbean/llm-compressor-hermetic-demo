# LLM Compressor Hermetic Build Demo

This repository demonstrates a hermetic build pipeline for LLM quantization using Konflux, showcasing:

- **Custom llm-compressor task** - A new Tekton task for running LLM quantization
- **x-huggingface support in hermeto** - Hermetic dependency fetching for Hugging Face models and datasets
- **One-shot quantization** - Compressing TinyLlama-1.1B-Chat-v1.0 using SmoothQuant + GPTQ

## Background

This demo is inspired by the [Merlinite PoC](https://github.com/ralphbean/merlinite-poc) and showcases integration of:

1. **llm-compressor task** - Custom Tekton task from [build-definitions/llm-compressor](https://github.com/ralphbean/build-definitions/tree/llm-compressor)
2. **hermeto x-huggingface** - ML dependency management from [hermeto PR #1141](https://github.com/hermetoproject/hermeto/pull/1141)

## Repository Structure

```
.
├── compress.py                    # Quantization script using llm-compressor
├── huggingface.lock.yaml         # Hermeto lockfile for HF models/datasets
├── .tekton/
│   ├── llm-compressor-pipeline.yaml  # Main pipeline definition
│   └── llm-compressor-push.yaml      # PipelineRun trigger config
├── konflux/
│   ├── application.yaml          # Konflux Application resource
│   ├── component.yaml            # Konflux Component resource
│   └── kustomization.yaml        # Kustomize config for easy deployment
└── README.md
```

## Prerequisites

1. **Konflux cluster** - Access to a Konflux instance
2. **Custom hermeto image** - Built from the x-huggingface branch:
   ```bash
   git clone https://github.com/hermetoproject/hermeto
   cd hermeto
   git fetch origin pull/1141/head:x-huggingface
   git checkout x-huggingface
   podman build -t quay.io/rbean/hermeto:x-huggingface .
   podman push quay.io/rbean/hermeto:x-huggingface
   ```
## Setup Instructions

### 1. Update Konflux Resources (if needed)

The resources are configured for the `konflux-ai-sig-tenant` namespace by default.

If you forked this repo or need a different namespace:
- Edit `konflux/kustomization.yaml` to set your namespace
- Edit `konflux/application.yaml` to update the repository URL
- Edit `konflux/component.yaml` to update source.git.url

### 2. Apply Konflux Resources

```bash
oc apply -k konflux/
```

### 3. Monitor Pipeline Execution

The pipeline will automatically trigger on push to main. Monitor with:

```bash
# Watch PipelineRuns
oc get pipelineruns -w

# View logs for a specific run
tkn pipelinerun logs llm-compressor-on-push-<run-id> -f
```

## Pipeline Stages

1. **init** - Initialize OCI Trusted Artifacts storage
2. **clone-repository** - Clone this Git repository using OCI-TA
3. **prefetch-dependencies** - Use hermeto to hermetically fetch:
   - TinyLlama/TinyLlama-1.1B-Chat-v1.0 model
   - garage-bAInd/Open-Platypus dataset
   - Artifacts passed via OCI registry (no PVC needed!)
4. **llm-compressor** - Run quantization using llm-compressor:
   - Apply SmoothQuant (smoothing_strength=0.8)
   - Apply GPTQ (W8A8 quantization)
   - Generate compressed model
5. **show-sbom** (finally) - Display Software Bill of Materials

## Expected Output

After successful pipeline execution:

- **Quantized model** - Stored in OCI artifacts at `quantized-model/`
- **SBOM** - Generated by hermeto showing all hermetically fetched dependencies
- **Logs** - Quantization metrics and model compression details

Note: This pipeline uses OCI Trusted Artifacts (OCI-TA) for passing data between tasks, eliminating the need for PersistentVolumeClaims!

## Key Components

### compress.py

Python script that uses llm-compressor to perform one-shot quantization:

```python
from llmcompressor.modifiers.smoothquant import SmoothQuantModifier
from llmcompressor.modifiers.quantization import GPTQModifier
from llmcompressor import oneshot

recipe = [
    SmoothQuantModifier(smoothing_strength=0.8),
    GPTQModifier(scheme="W8A8", targets="Linear", ignore=["lm_head"]),
]

oneshot(
    model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    dataset="open_platypus",
    recipe=recipe,
    output_dir="TinyLlama-1.1B-Chat-v1.0-INT8",
    max_seq_length=2048,
    num_calibration_samples=512,
)
```

### huggingface.lock.yaml

Hermeto lockfile specifying exact versions of ML dependencies. Both models and datasets are declared in the `models` list with a `type` field to distinguish them:

```yaml
metadata:
  version: "1.0"

models:
  - repository: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    revision: "fe8a4ea1ffedaf415f4da2f062534de366a451e6"
    type: "model"
    include_patterns:
      - "*.safetensors"
      - "config.json"
      - "tokenizer*"

  - repository: "garage-bAInd/Open-Platypus"
    revision: "37141edbdb7826378cce118c46a109b813e1f038"
    type: "dataset"
    include_patterns:
      - "*.parquet"
      - "*.json"
```

## Troubleshooting

### Pipeline fails at prefetch-dependencies

- Verify hermeto image is built and pushed: `podman pull quay.io/rbean/hermeto:x-huggingface`
- Verify OCI storage is configured correctly in your Konflux instance
- Check network access to huggingface.co from your cluster

### Pipeline fails at llm-compressor

- Verify llm-compressor task is available in your Konflux instance
- Check model-opt-cuda image is accessible: `podman pull registry.redhat.io/rhaiis/model-opt-cuda-rhel9:3.2.3-1760571858`
- Review CPU/memory resource allocations

### SBOM not showing dependencies

- Verify hermeto generated SBOM during prefetch stage
- Check task logs for hermeto SBOM output

## References

- [llm-compressor](https://github.com/vllm-project/llm-compressor) - LLM quantization library
- [hermeto](https://github.com/hermetoproject/hermeto) - Hermetic dependency management
- [build-definitions](https://github.com/konflux-ci/build-definitions) - Konflux task catalog
- [Merlinite PoC](https://github.com/ralphbean/merlinite-poc) - Previous demo

## Author

Ralph Bean (rbean@redhat.com)

## License

Apache 2.0
